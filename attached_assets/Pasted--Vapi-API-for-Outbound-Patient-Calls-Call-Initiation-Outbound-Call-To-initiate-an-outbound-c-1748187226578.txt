# Vapi API for Outbound Patient Calls

## Call Initiation (Outbound Call)

To initiate an outbound call via Vapi, use the **POST** `​/call/phone` endpoint with a JSON payload containing three key pieces: the assistant to use, the caller ID number, and the destination number. You can either reference an existing **assistant** by its ID or define a **transient assistant** configuration inline. Include the `phoneNumberId` of your registered outbound number (or use `phoneNumber` for a transient caller ID) and provide the patient’s phone number (or SIP URI) in the `customer` object. For example, a minimal request to have an assistant call a patient might look like:

```json
POST /call/phone  HTTP/1.1
Content-Type: application/json
Authorization: Bearer <YOUR_API_KEY>

{
  "assistantId": "assistant-id",         // or use "assistant": { ... } for transient config
  "phoneNumberId": "phone-number-id",    // the ID of your Vapi or integrated phone number
  "customer": {
    "number": "+11231231234"             // patient’s phone number in E.164 format
  }
}
```

In this example, the assistant with ID `"assistant-id"` will call the number `+1 1231231234` using the specified caller ID number. To use a **transient assistant** instead of a saved one, omit `assistantId` and provide an `assistant` object in the payload (as detailed below). Vapi requires **either** `assistantId` or an `assistant` definition in the request – if an `assistant` object is given, it will override any saved assistant settings for this call.

## Assistant Configuration at Call Time

When dynamically configuring the AI assistant for each call, include an `assistant` object in the POST request. This **transient assistant** can be fully customized with voice and behavior settings at call time. The assistant object supports fields such as:

- **firstMessage** – the initial phrase the assistant will speak when the call connects (can also be a URL to an audio file). If not set, the call will start with silence and wait for the patient to speak.
    
- **voice** – the text-to-speech voice configuration, including the TTS provider and the specific voice ID. This controls how the assistant sounds (e.g. gender, accent).
    
- **model** – the language model settings, including the LLM provider, model name, and a **system prompt** (provided as a `system` role message) that defines the assistant’s role/personality and instructions.
    
- **transcriber** – the speech-to-text provider and model for transcribing the caller’s speech.
    

For example, you can define a transient assistant inline with a custom voice, system prompt, and first message:

```json
"assistant": {
  "name": "NurseAssist", 
  "firstMessage": "Hi, this is NurseAssist. I'm calling with your health update.",
  "voice": {
    "provider": "aws",
    "voiceId": "Joanna"
  },
  "model": {
    "provider": "openai",
    "model": "gpt-4",
    "messages": [
      {
        "role": "system",
        "content": "You are a friendly medical assistant calling a patient. Speak clearly and compassionately, and provide health guidance based on the patient’s records."
      }
    ]
  },
  "transcriber": {
    "provider": "google",
    "model": "default"
  }
}
```

In the above payload, the assistant will start the call by saying “Hi, this is NurseAssist…health update.” as specified by **firstMessage**, using Amazon Polly voice “Joanna” for text-to-speech. The system prompt (as a `system` message under **model**) gives the AI its role and context during the call. All of these values are set per call and do not require creating a permanent assistant in the Vapi dashboard. (If you are using a saved assistant via `assistantId`, note that you can still override certain settings at call time with `assistantOverrides` as described next.)

## Passing Patient-Specific Data to the Assistant

You can inject patient-specific information (e.g. name, health conditions, recent visit details) into the call by using **dynamic variables** in your assistant’s prompts and providing their values in the API request. In your assistant’s system prompt or messages, use placeholders like `{{name}}` or `{{condition}}`. Then, when making the call request, supply an `assistantOverrides.variableValues` object mapping those variables to actual values. (This is only done via API/SDK; you cannot set these dynamic values in the dashboard UI.)

For example, if the assistant’s first message is defined as, “Hello, **{{name}}**, I’m calling with an update on your **{{condition}}**,” you can pass the patient’s name and condition at call time:

```json
{
  "assistantId": "your-assistant-id",
  "assistantOverrides": {
    "variableValues": { 
      "name": "John Doe",
      "condition": "blood pressure"
    }
  },
  "customer": {
    "number": "+1xxxxxxxxxx"
  },
  "phoneNumberId": "your-phone-id"
}
```

In this JSON payload, the `assistantOverrides.variableValues` injects **John Doe** and **blood pressure** into the assistant’s prompt before the call starts. Vapi will replace the `{{name}}` and `{{condition}}` tokens in your assistant’s first message or system prompt with “John Doe” and “blood pressure” respectively during the call. This allows each call to be personalized with patient-specific data (such as triage notes, current health conditions, or conversational cues) provided by your frontend UI.

## Retrieving Call Summaries and Transcripts

After a call completes, you can obtain a summary or transcript of the conversation either by polling the call status or by using webhooks (server callbacks):

- **Polling:** Use the **GET** `​/call/{callId}` endpoint to fetch the call record once its status is `"completed"`. The call resource includes analysis results such as a conversational **summary** and any captured structured data. For example, the call object contains fields like `"summary"` (a text summary of the call) and `"structuredData"` (JSON with key findings) once the call has ended. You can retrieve the full transcript by aggregating the conversation messages if needed, or by enabling transcript logging (see observability settings).
    
- **Webhooks (Server URLs):** For real-time notifications, configure a **Server URL** in Vapi’s dashboard or via API to receive events. Vapi will send an **“end-of-call-report”** HTTP POST to your server as soon as a call finishes. The webhook’s JSON payload contains a `message` object with a `type: "end-of-call-report"` and includes the call’s summary and transcript, along with metadata (e.g. call ID, end reason). For example, your server might receive:
    
    ```json
    {
      "message": {
        "type": "end-of-call-report",
        "summary": "The patient asked about medication dosage and was advised to rest and hydrate.",
        "transcript": "Assistant: Hello, I'm calling from... \nPatient: I have a question about...\nAssistant: ...", 
        "endedReason": "hangup",
        "call": { "...": "..." }
      }
    }
    ```
    
    This **End-of-Call Report** provides a concise summary of the conversation (and the full dialogue transcript if enabled) so you can log it or display it in your application. You should implement a handler on your server to capture these webhook POSTs for post-call analysis. (Ensure your server endpoint is reachable and configured in Vapi as a Server URL.)
    

## Voice Configuration Sandbox (Previewing Voices)

To let nurses preview and fine-tune the assistant’s voice settings, you can leverage the same API. Nurses can select different voice options (TTS providers and voice IDs), and you can trigger a quick outbound call to a test number (e.g. the nurse’s own phone or a demo line) with the chosen configuration. Simply include the desired `voice` in the call’s assistant definition and provide a suitable `firstMessage` for the preview. For example, to test a custom voice you might send:

```json
{
  "assistant": {
    "firstMessage": "This is a voice preview using the new configuration.",
    "voice": {
      "provider": "deepgram",
      "voiceId": "your-voice-id"
    },
    "model": { ... }, 
    "transcriber": { ... }
  },
  "phoneNumberId": "<your-number-id>",
  "customer": { "number": "<nurse-phone-number>" }
}
```

In this request, the assistant will call the nurse’s number and speak the provided firstMessage in the specified voice. The `voice` property is where you control the TTS voice; for instance, you might set `"provider": "aws"` and `"voiceId": "Matthew"` for an English male voice, or another provider like ElevenLabs with a custom voice ID. This “sandbox” approach allows iterative testing – nurses can listen and decide if the voice and tone fit their needs.

**Tip:** You can programmatically **update the assistant’s saved settings** once a voice or prompt is finalized. For example, using the **Update Assistant** endpoint, you could persist the new voiceId or firstMessage so that subsequent calls use the updated configuration by default. However, for on-the-fly previews or one-off calls, using transient assistant settings (as shown above) is usually simplest.

By using the Vapi API endpoints in this way, you can fully control outbound calls and assistant behavior from your frontend application – selecting or customizing the AI assistant per call, injecting patient-specific context, and retrieving call results for review. All requests and responses are JSON-based, making it easy for developers and AI tools to integrate immediately with the Vapi platform.
